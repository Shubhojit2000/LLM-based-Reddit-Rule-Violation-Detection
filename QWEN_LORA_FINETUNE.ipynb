{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06007c27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:56:20.564915Z",
     "iopub.status.busy": "2025-09-30T19:56:20.564669Z",
     "iopub.status.idle": "2025-09-30T19:56:20.572447Z",
     "shell.execute_reply": "2025-09-30T19:56:20.571562Z"
    },
    "papermill": {
     "duration": 0.012714,
     "end_time": "2025-09-30T19:56:20.573751",
     "exception": false,
     "start_time": "2025-09-30T19:56:20.561037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing constants.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile constants.py\n",
    "# Define paths and constants for the training process\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1\"\n",
    "LORA_PATH = \"/kaggle/working/\" # The directory where the trained LoRA adapter will be saved\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\"\n",
    "COMPLETE_PHRASE = \"Answer:\"\n",
    "BASE_PROMPT = '''You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4eab3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:56:20.580343Z",
     "iopub.status.busy": "2025-09-30T19:56:20.580163Z",
     "iopub.status.idle": "2025-09-30T19:56:20.585892Z",
     "shell.execute_reply": "2025-09-30T19:56:20.585336Z"
    },
    "papermill": {
     "duration": 0.009806,
     "end_time": "2025-09-30T19:56:20.586961",
     "exception": false,
     "start_time": "2025-09-30T19:56:20.577155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT\n",
    "import random, numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def build_prompt(row):\n",
    "    \"\"\"\n",
    "    Constructs the prompt for the language model with few-shot examples.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "def get_dataframe_to_train(data_path):\n",
    "    \"\"\"\n",
    "    Prepares the training and testing data by creating a balanced dataset of\n",
    "    rule violations and non-violations, including positive and negative examples.\n",
    "    \"\"\"\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    flatten = []\n",
    "\n",
    "    # Process the training set\n",
    "    train_df = train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n",
    "                              \"positive_example_1\",\"positive_example_2\",\n",
    "                              \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "    # Randomly select one positive and one negative example\n",
    "    train_df[\"positive_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"positive_example_1\"],\n",
    "        train_df[\"positive_example_2\"]\n",
    "    )\n",
    "    train_df[\"negative_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"negative_example_1\"],\n",
    "        train_df[\"negative_example_2\"]\n",
    "    )\n",
    "\n",
    "    # Drop original example columns\n",
    "    train_df.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                           \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "    flatten.append(train_df)\n",
    "\n",
    "    # Process the test set to augment training data\n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            sub_dataset = test_dataset[[\"rule\",\"subreddit\",\n",
    "                                        \"positive_example_1\",\"positive_example_2\",\n",
    "                                        \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "            if violation_type == \"positive\":\n",
    "                # Use a positive example as the comment body\n",
    "                body_col = f\"positive_example_{i}\"\n",
    "                other_positive_col = f\"positive_example_{3-i}\"\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n",
    "                # Randomly select a negative example\n",
    "                sub_dataset[\"negative_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"negative_example_1\"],\n",
    "                    sub_dataset[\"negative_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 1\n",
    "\n",
    "            else:  # violation_type == \"negative\"\n",
    "                # Use a negative example as the comment body\n",
    "                body_col = f\"negative_example_{i}\"\n",
    "                other_negative_col = f\"negative_example_{3-i}\"\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n",
    "                # Randomly select a positive example\n",
    "                sub_dataset[\"positive_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"positive_example_1\"],\n",
    "                    sub_dataset[\"positive_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 0\n",
    "\n",
    "            # Drop original example columns\n",
    "            sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                                      \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "            flatten.append(sub_dataset)\n",
    "\n",
    "    # Combine all dataframes\n",
    "    dataframe = pd.concat(flatten, axis=0)\n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def build_dataset(dataframe):\n",
    "    \"\"\"\n",
    "    Builds a dataset suitable for training and inference,\n",
    "    including the prompt and completion pairs.\n",
    "    \"\"\"\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    columns = [\"prompt\"]\n",
    "    if \"rule_violation\" in dataframe:\n",
    "        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: POSITIVE_ANSWER,\n",
    "                0: NEGATIVE_ANSWER,\n",
    "            }\n",
    "        )\n",
    "        columns.append(\"completion\")\n",
    "\n",
    "    dataframe = dataframe[columns]\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bdc36ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:56:20.592607Z",
     "iopub.status.busy": "2025-09-30T19:56:20.592052Z",
     "iopub.status.idle": "2025-09-30T19:56:20.596975Z",
     "shell.execute_reply": "2025-09-30T19:56:20.596299Z"
    },
    "papermill": {
     "duration": 0.008739,
     "end_time": "2025-09-30T19:56:20.598049",
     "exception": false,
     "start_time": "2025-09-30T19:56:20.589310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from utils import build_dataset, get_dataframe_to_train\n",
    "from constants import DATA_PATH, BASE_MODEL_PATH, LORA_PATH\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to train the LoRA model.\n",
    "    \"\"\"\n",
    "    dataframe = get_dataframe_to_train(DATA_PATH)\n",
    "    train_dataset = build_dataset(dataframe)\n",
    "    \n",
    "    # LoRA configuration for parameter-efficient fine-tuning\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    # SFTConfig for supervised fine-tuning\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=LORA_PATH, # Specify the output directory for checkpoints etc.\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        bf16=is_torch_bf16_gpu_available(),\n",
    "        fp16=not is_torch_bf16_gpu_available(),\n",
    "        dataloader_pin_memory=True,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        save_strategy=\"no\", # We will save manually at the end\n",
    "        report_to=\"none\",\n",
    "        completion_only_loss=True,\n",
    "        packing=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    # Initialize the trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=BASE_MODEL_PATH,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    print(\"Training finished.\")\n",
    "    \n",
    "    # Save the trained LoRA adapter\n",
    "    trainer.save_model(LORA_PATH)\n",
    "    print(f\"LoRA adapter saved to {LORA_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63822106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:56:20.603672Z",
     "iopub.status.busy": "2025-09-30T19:56:20.603157Z",
     "iopub.status.idle": "2025-09-30T19:56:20.607852Z",
     "shell.execute_reply": "2025-09-30T19:56:20.607126Z"
    },
    "papermill": {
     "duration": 0.008604,
     "end_time": "2025-09-30T19:56:20.608988",
     "exception": false,
     "start_time": "2025-09-30T19:56:20.600384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing accelerate_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_config.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 4\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 64\n",
    "  train_micro_batch_size_per_gpu: 4\n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: INDUCTOR\n",
    "  dynamo_use_fullgraph: false\n",
    "  dynamo_use_dynamic: false\n",
    "enable_cpu_affinity: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 2\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9c2834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:56:20.614540Z",
     "iopub.status.busy": "2025-09-30T19:56:20.614101Z",
     "iopub.status.idle": "2025-09-30T20:06:49.803292Z",
     "shell.execute_reply": "2025-09-30T20:06:49.802507Z"
    },
    "papermill": {
     "duration": 629.193477,
     "end_time": "2025-09-30T20:06:49.804773",
     "exception": false,
     "start_time": "2025-09-30T19:56:20.611296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-30 19:56:30,407] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2025-09-30 19:56:33,235] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\r\n",
      "2025-09-30 19:56:34.814400: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1759262195.021295     187 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1759262195.086753     187 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0930 19:56:47.045000 187 torch/distributed/run.py:766] \r\n",
      "W0930 19:56:47.045000 187 torch/distributed/run.py:766] *****************************************\r\n",
      "W0930 19:56:47.045000 187 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W0930 19:56:47.045000 187 torch/distributed/run.py:766] *****************************************\r\n",
      "[W930 19:56:55.159794543 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W930 19:57:03.162300150 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "2025-09-30 19:57:09.492337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1759262229.515600     244 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1759262229.522393     244 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-09-30 19:57:09.531038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1759262229.553662     243 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1759262229.560575     243 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "[2025-09-30 19:57:16,345] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2025-09-30 19:57:16,345] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2025-09-30 19:57:17,572] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\r\n",
      "[2025-09-30 19:57:17,587] [INFO] [comm.py:821:init_distributed] cdb=None\r\n",
      "[2025-09-30 19:57:18,095] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\r\n",
      "[2025-09-30 19:57:18,119] [INFO] [comm.py:821:init_distributed] cdb=None\r\n",
      "[2025-09-30 19:57:18,119] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n",
      "[W930 19:57:25.700222817 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W930 19:57:26.233056172 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W930 19:57:33.702679155 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W930 19:57:41.704691300 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n",
      "  @custom_fwd\r\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n",
      "  @custom_bwd\r\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n",
      "  @custom_fwd(cast_inputs=torch.float16)\r\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n",
      "  @custom_fwd\r\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n",
      "  @custom_bwd\r\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n",
      "  @custom_fwd(cast_inputs=torch.float16)\r\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\r\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\r\n",
      "Adding EOS to train dataset: 100%|█| 2049/2049 [00:00<00:00, 11810.26 examples/s\r\n",
      "Tokenizing train dataset: 100%|█████| 2049/2049 [00:03<00:00, 556.56 examples/s]\r\n",
      "Truncating train dataset: 100%|███| 2049/2049 [00:00<00:00, 48968.00 examples/s]\r\n",
      "Adding EOS to train dataset:   0%|              | 0/2049 [00:00<?, ? examples/s]Starting model training...\r\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\r\n",
      "Adding EOS to train dataset: 100%|█| 2049/2049 [00:00<00:00, 22891.00 examples/s\r\n",
      "Tokenizing train dataset: 100%|█████| 2049/2049 [00:02<00:00, 710.48 examples/s]\r\n",
      "Truncating train dataset: 100%|██| 2049/2049 [00:00<00:00, 208403.15 examples/s]\r\n",
      "Starting model training...\r\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\r\n",
      "[2025-09-30 19:57:54,765] [WARNING] [engine.py:1373:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\r\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\r\n",
      "  0%|                                                    | 0/65 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\r\n",
      "{'loss': 5.9161, 'grad_norm': 2.399785280227661, 'learning_rate': 9.698463103929542e-05, 'num_tokens': 68410.0, 'mean_token_accuracy': 0.3125, 'epoch': 0.16}\r\n",
      "{'loss': 0.3709, 'grad_norm': 2.515498399734497, 'learning_rate': 8.308429187984297e-05, 'num_tokens': 136025.0, 'mean_token_accuracy': 0.79375, 'epoch': 0.31}\r\n",
      "{'loss': 0.363, 'grad_norm': 1.6847258806228638, 'learning_rate': 6.112604669781572e-05, 'num_tokens': 204188.0, 'mean_token_accuracy': 0.7828125, 'epoch': 0.47}\r\n",
      "{'loss': 0.3316, 'grad_norm': 3.483509063720703, 'learning_rate': 3.6457976592849754e-05, 'num_tokens': 271365.0, 'mean_token_accuracy': 0.803125, 'epoch': 0.62}\r\n",
      "{'loss': 0.3296, 'grad_norm': 1.5397591590881348, 'learning_rate': 1.5088159095696363e-05, 'num_tokens': 338855.0, 'mean_token_accuracy': 0.8125, 'epoch': 0.78}\r\n",
      "{'loss': 0.3038, 'grad_norm': 1.7506310939788818, 'learning_rate': 2.221359710692961e-06, 'num_tokens': 407756.0, 'mean_token_accuracy': 0.8390625, 'epoch': 0.93}\r\n",
      "{'train_runtime': 524.7208, 'train_samples_per_second': 3.905, 'train_steps_per_second': 0.124, 'train_loss': 1.1945329996255727, 'num_tokens': 434309.0, 'mean_token_accuracy': 0.8198529411764706, 'epoch': 1.0}\r\n",
      "Training finished.\r\n",
      "100%|███████████████████████████████████████████| 65/65 [08:44<00:00,  8.07s/it]\r\n",
      "Training finished.\r\n",
      "LoRA adapter saved to /kaggle/working/\r\n",
      "LoRA adapter saved to /kaggle/working/\r\n",
      "[rank0]:[W930 20:06:44.602489152 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"
     ]
    }
   ],
   "source": [
    "# Launch the training process using accelerate\n",
    "!accelerate launch --config_file accelerate_config.yaml train.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "datasetId": 8067935,
     "sourceId": 12762469,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8379360,
     "isSourceIdPinned": true,
     "sourceId": 13220685,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 145960,
     "sourceId": 171496,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 146086,
     "sourceId": 171638,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 368803,
     "modelInstanceId": 347541,
     "sourceId": 426330,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 429004,
     "modelInstanceId": 411182,
     "sourceId": 523492,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 633.518352,
   "end_time": "2025-09-30T20:06:50.238867",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-30T19:56:16.720515",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

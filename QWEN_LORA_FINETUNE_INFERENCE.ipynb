{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4fcbf55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:18:12.732659Z",
     "iopub.status.busy": "2025-09-30T20:18:12.732285Z",
     "iopub.status.idle": "2025-09-30T20:18:12.738806Z",
     "shell.execute_reply": "2025-09-30T20:18:12.738197Z"
    },
    "papermill": {
     "duration": 0.010106,
     "end_time": "2025-09-30T20:18:12.739857",
     "exception": false,
     "start_time": "2025-09-30T20:18:12.729751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing constants.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile constants.py\n",
    "# IMPORTANT: Update LORA_PATH to point to your saved adapter dataset\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1\"\n",
    "LORA_PATH = \"/kaggle/input/qwen2-5-lora/\" # <-- CHANGE THIS to the path of your Kaggle dataset\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\"\n",
    "COMPLETE_PHRASE = \"Answer:\"\n",
    "BASE_PROMPT = '''You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4450e6aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:18:12.745300Z",
     "iopub.status.busy": "2025-09-30T20:18:12.744733Z",
     "iopub.status.idle": "2025-09-30T20:18:12.749152Z",
     "shell.execute_reply": "2025-09-30T20:18:12.748480Z"
    },
    "papermill": {
     "duration": 0.008245,
     "end_time": "2025-09-30T20:18:12.750321",
     "exception": false,
     "start_time": "2025-09-30T20:18:12.742076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT\n",
    "import random, numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    \"\"\"\n",
    "    Constructs the prompt for the language model with few-shot examples.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "\n",
    "def build_dataset(dataframe):\n",
    "    \"\"\"\n",
    "    Builds a dataset suitable for inference.\n",
    "    \"\"\"\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "    dataset = Dataset.from_pandas(dataframe[[\"prompt\"]])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3a69db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:18:12.755715Z",
     "iopub.status.busy": "2025-09-30T20:18:12.755501Z",
     "iopub.status.idle": "2025-09-30T20:18:12.761288Z",
     "shell.execute_reply": "2025-09-30T20:18:12.760602Z"
    },
    "papermill": {
     "duration": 0.009787,
     "end_time": "2025-09-30T20:18:12.762350",
     "exception": false,
     "start_time": "2025-09-30T20:18:12.752563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "import vllm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "from vllm.lora.request import LoRARequest\n",
    "from utils import build_dataset\n",
    "from constants import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def run_inference_on_device(df_slice):\n",
    "    \"\"\"\n",
    "    Runs vLLM inference on a specific GPU using the pre-trained LoRA adapter.\n",
    "    \"\"\"\n",
    "    llm = vllm.LLM(\n",
    "        BASE_MODEL_PATH,\n",
    "        quantization=\"gptq\",\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.98,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2836,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "        max_lora_rank=64,\n",
    "    )\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    # Constrain the output to be either \"Yes\" or \"No\"\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POSITIVE_ANSWER, NEGATIVE_ANSWER])\n",
    "\n",
    "    test_dataset = build_dataset(df_slice)\n",
    "    texts = test_dataset[\"prompt\"]\n",
    "\n",
    "    # Generate predictions using the specified LoRA adapter\n",
    "    outputs = llm.generate(\n",
    "        texts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logits_processors=[mclp],\n",
    "            logprobs=2,\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\n",
    "            lora_name=\"default\", # A unique name for the LoRA request\n",
    "            lora_int_id=1,      # A unique integer ID for the LoRA adapter\n",
    "            lora_local_path=LORA_PATH # Path to the saved LoRA adapter\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Extract log probabilities\n",
    "    log_probs = [\n",
    "        {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n",
    "        for out in outputs\n",
    "    ]\n",
    "    predictions = pd.DataFrame(log_probs)[[POSITIVE_ANSWER, NEGATIVE_ANSWER]]\n",
    "    predictions[\"row_id\"] = df_slice[\"row_id\"].values\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def worker(device_id, df_slice, return_dict):\n",
    "    \"\"\"\n",
    "    Worker process to run inference on a slice of the data.\n",
    "    \"\"\"\n",
    "    # Set the visible GPU for this process\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n",
    "    print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n",
    "\n",
    "    preds = run_inference_on_device(df_slice)\n",
    "    return_dict[device_id] = preds\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for parallelized inference.\n",
    "    \"\"\"\n",
    "    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "\n",
    "    # Randomly select one positive and one negative example for the few-shot prompt\n",
    "    test_dataframe[\"positive_example\"] = test_dataframe.apply(\n",
    "        lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]),\n",
    "        axis=1\n",
    "    )\n",
    "    test_dataframe[\"negative_example\"] = test_dataframe.apply(\n",
    "        lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]),\n",
    "        axis=1\n",
    "    )\n",
    "    test_dataframe = test_dataframe.drop(\n",
    "        columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"],\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    # Split the data for parallel processing\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 1:\n",
    "        mid = len(test_dataframe) // num_gpus\n",
    "        dfs = [test_dataframe.iloc[i*mid:(i+1)*mid].reset_index(drop=True) for i in range(num_gpus-1)]\n",
    "        dfs.append(test_dataframe.iloc[(num_gpus-1)*mid:].reset_index(drop=True))\n",
    "    else:\n",
    "        dfs = [test_dataframe]\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    return_dict = manager.dict()\n",
    "\n",
    "    processes = [mp.Process(target=worker, args=(i, df, return_dict)) for i, df in enumerate(dfs)]\n",
    "    for p in processes:\n",
    "        p.start()\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    # Combine the results\n",
    "    predictions = pd.concat([return_dict[i] for i in range(len(dfs))], ignore_index=True)\n",
    "\n",
    "    # Create the submission file\n",
    "    submission = predictions[[\"row_id\", POSITIVE_ANSWER]].rename(columns={POSITIVE_ANSWER: \"rule_violation\"})\n",
    "    \n",
    "    # Normalize probabilities to ranks for a robust submission format\n",
    "    submission['rule_violation'] = submission['rule_violation'].rank(method='average') / (len(submission) + 1)\n",
    "\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"✅ Saved submission.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dc4882a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:18:12.767565Z",
     "iopub.status.busy": "2025-09-30T20:18:12.767376Z",
     "iopub.status.idle": "2025-09-30T20:19:57.354288Z",
     "shell.execute_reply": "2025-09-30T20:19:57.353568Z"
    },
    "papermill": {
     "duration": 104.591107,
     "end_time": "2025-09-30T20:19:57.355808",
     "exception": false,
     "start_time": "2025-09-30T20:18:12.764701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Worker 0] Running on GPU 0, data size=5\r\n",
      "[Worker 1] Running on GPU 1, data size=5\r\n",
      "2025-09-30 20:18:25.764893: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-09-30 20:18:25.764893: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1759263506.135834     202 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1759263506.135972     204 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1759263506.244833     202 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1759263506.244849     204 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 09-30 20:18:43 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "INFO 09-30 20:18:43 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\r\n",
      "INFO 09-30 20:19:00 [config.py:1604] Using max model len 2836\r\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\r\n",
      "INFO 09-30 20:19:00 [config.py:1604] Using max model len 2836\r\n",
      "WARNING 09-30 20:19:01 [config.py:1084] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "WARNING 09-30 20:19:01 [config.py:1084] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "WARNING 09-30 20:19:02 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "WARNING 09-30 20:19:02 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 09-30 20:19:02 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2836, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \r\n",
      "INFO 09-30 20:19:02 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2836, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \r\n",
      "INFO 09-30 20:19:03 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 09-30 20:19:03 [cuda.py:395] Using XFormers backend.\r\n",
      "INFO 09-30 20:19:03 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 09-30 20:19:03 [cuda.py:395] Using XFormers backend.\r\n",
      "[W930 20:19:14.280281928 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W930 20:19:14.280437809 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W930 20:19:24.291040180 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W930 20:19:24.302058302 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 09-30 20:19:24 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\r\n",
      "INFO 09-30 20:19:24 [model_runner.py:1083] Starting to load model /kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1...\r\n",
      "INFO 09-30 20:19:24 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\r\n",
      "INFO 09-30 20:19:24 [model_runner.py:1083] Starting to load model /kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.41s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.41s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.41s/it]\r\n",
      "\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.41s/it]\r\n",
      "\r\n",
      "INFO 09-30 20:19:33 [default_loader.py:262] Loading weights took 8.42 seconds\r\n",
      "INFO 09-30 20:19:33 [logger.py:65] Using PunicaWrapperGPU.\r\n",
      "INFO 09-30 20:19:33 [default_loader.py:262] Loading weights took 8.46 seconds\r\n",
      "INFO 09-30 20:19:33 [logger.py:65] Using PunicaWrapperGPU.\r\n",
      "INFO 09-30 20:19:34 [model_runner.py:1115] Model loading took 0.4969 GiB and 8.674251 seconds\r\n",
      "INFO 09-30 20:19:34 [model_runner.py:1115] Model loading took 0.4969 GiB and 8.717086 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 09-30 20:19:42 [worker.py:295] model weights take 0.50GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 12.51GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 09-30 20:19:42 [worker.py:295] model weights take 0.50GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 12.51GiB.\r\n",
      "INFO 09-30 20:19:42 [executor_base.py:113] # cuda blocks: 68319, # CPU blocks: 21845\r\n",
      "INFO 09-30 20:19:42 [executor_base.py:118] Maximum concurrency for 2836 tokens per request: 385.44x\r\n",
      "INFO 09-30 20:19:42 [executor_base.py:113] # cuda blocks: 68319, # CPU blocks: 21845\r\n",
      "INFO 09-30 20:19:42 [executor_base.py:118] Maximum concurrency for 2836 tokens per request: 385.44x\r\n",
      "INFO 09-30 20:19:48 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 13.81 seconds\r\n",
      "INFO 09-30 20:19:48 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 14.13 seconds\r\n",
      "/kaggle/working/inference.py:51: DeprecationWarning: The 'lora_local_path' attribute is deprecated and will be removed in a future version. Please use 'lora_path' instead.\r\n",
      "  lora_request=LoRARequest(\r\n",
      "Adding requests:   0%|                                    | 0/5 [00:00<?, ?it/s]/kaggle/working/inference.py:51: DeprecationWarning: The 'lora_local_path' attribute is deprecated and will be removed in a future version. Please use 'lora_path' instead.\r\n",
      "  lora_request=LoRARequest(\r\n",
      "Adding requests: 100%|████████████████████████████| 5/5 [00:00<00:00,  5.81it/s]\r\n",
      "Adding requests: 100%|████████████████████████████| 5/5 [00:00<00:00,  6.68it/s]\r\n",
      "Processed prompts: 100%|█| 5/5 [00:00<00:00,  6.90it/s, est. speed input: 1608.5\r\n",
      "Processed prompts: 100%|█| 5/5 [00:00<00:00,  6.88it/s, est. speed input: 1373.7\r\n",
      "✅ Saved submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Run the inference script\n",
    "!python inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbd4c91c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:19:57.365876Z",
     "iopub.status.busy": "2025-09-30T20:19:57.365241Z",
     "iopub.status.idle": "2025-09-30T20:19:57.485976Z",
     "shell.execute_reply": "2025-09-30T20:19:57.485221Z"
    },
    "papermill": {
     "duration": 0.126896,
     "end_time": "2025-09-30T20:19:57.487180",
     "exception": false,
     "start_time": "2025-09-30T20:19:57.360284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id,rule_violation\r\n",
      "2029,0.13636363636363635\r\n",
      "2030,0.7272727272727273\r\n",
      "2031,0.6363636363636364\r\n",
      "2032,0.13636363636363635\r\n",
      "2033,0.45454545454545453\r\n",
      "2034,0.5454545454545454\r\n",
      "2035,0.8181818181818182\r\n",
      "2036,0.36363636363636365\r\n",
      "2037,0.2727272727272727\r\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the submission file\n",
    "!head submission.csv"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "datasetId": 8067935,
     "sourceId": 12762469,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8379360,
     "isSourceIdPinned": true,
     "sourceId": 13221065,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 145960,
     "sourceId": 171496,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 146086,
     "sourceId": 171638,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 368803,
     "modelInstanceId": 347541,
     "sourceId": 426330,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 429004,
     "modelInstanceId": 411182,
     "sourceId": 523492,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 110.128105,
   "end_time": "2025-09-30T20:19:57.707794",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-30T20:18:07.579689",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
